{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naveed/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: failed to get driver name for fd -1\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to retrieve device information\n",
      "\n",
      "libEGL warning: failed to get driver name for fd -1\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1725538408.065595   54106 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1725538408.072677   62325 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1725538408.285752   62310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1725538408.474274   62310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naveed/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 35 persons, 2 birds, 237.8ms\n",
      "Speed: 8.2ms preprocess, 237.8ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naveed/.local/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 34 persons, 3 birds, 1037.1ms\n",
      "Speed: 4.2ms preprocess, 1037.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 3 birds, 210.6ms\n",
      "Speed: 4.8ms preprocess, 210.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 birds, 226.4ms\n",
      "Speed: 4.4ms preprocess, 226.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 2 birds, 200.2ms\n",
      "Speed: 7.8ms preprocess, 200.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 37 persons, 2 birds, 207.0ms\n",
      "Speed: 5.7ms preprocess, 207.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 3 birds, 242.1ms\n",
      "Speed: 3.6ms preprocess, 242.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 35 persons, 2 birds, 299.2ms\n",
      "Speed: 7.2ms preprocess, 299.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 birds, 247.4ms\n",
      "Speed: 4.7ms preprocess, 247.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 36 persons, 2 birds, 262.6ms\n",
      "Speed: 4.9ms preprocess, 262.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 birds, 254.2ms\n",
      "Speed: 5.7ms preprocess, 254.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 2 birds, 205.8ms\n",
      "Speed: 4.4ms preprocess, 205.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 33 persons, 2 birds, 221.9ms\n",
      "Speed: 3.5ms preprocess, 221.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 34 persons, 3 birds, 228.6ms\n",
      "Speed: 4.4ms preprocess, 228.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize YOLOv8n model (pre-trained)\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open video capture (replace 'path_to_video.mp4' with the actual video file path)\n",
    "video_path = 'Test.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8n Detection\n",
    "    results = yolo_model(frame)\n",
    "    detections = results[0].boxes.xyxy  # Bounding boxes in xyxy format\n",
    "\n",
    "    # Loop through detections\n",
    "    for box in detections:\n",
    "        x1, y1, x2, y2 = map(int, box)  # Convert to integer\n",
    "\n",
    "        # Crop detected patient region\n",
    "        patient_region = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert patient region to RGB format for MediaPipe\n",
    "        rgb_patient = cv2.cvtColor(patient_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # MediaPipe Pose Estimation\n",
    "        result = pose.process(rgb_patient)\n",
    "\n",
    "        # Draw pose landmarks if detected\n",
    "        if result.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(patient_region, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Put the cropped region back in the original frame\n",
    "        frame[y1:y2, x1:x2] = patient_region\n",
    "\n",
    "    # Display the output frame\n",
    "    cv2.imshow('Patient Detection and Pose Estimation', frame)\n",
    "\n",
    "    # Break loop with 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
